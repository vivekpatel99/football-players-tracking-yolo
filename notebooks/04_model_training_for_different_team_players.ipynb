{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f9033",
   "metadata": {},
   "source": [
    "# Perform inference with trained yolo model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6e540",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e178f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.dirname(os.getcwd()),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "if os.getenv(\"DATA_ROOT\") is None:\n",
    "    os.environ[\"DATA_ROOT\"] = f\"{root}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5578c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import hydra\n",
    "import numpy as np\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8e5fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Found!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hydra import compose, initialize\n",
    "\n",
    "# Setup device-agnostic code\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # NVIDIA GPU\n",
    "    print(\"GPU Found!!\")\n",
    "else:\n",
    "    raise Exception(\"No GPU Found!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21966ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc113a1",
   "metadata": {},
   "source": [
    "## Paths setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8813a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root_dir': '${oc.env:PROJECT_ROOT}', 'root_data_dir': '${oc.env:DATA_ROOT}', 'results_dir': '${paths.root_dir}/results', 'log_dir': '${paths.root_dir}/logs/', 'output_dir': '${hydra:runtime.output_dir}', 'work_dir': '${hydra:runtime.cwd}', 'pretrained_model_dir': '${paths.root_dir}/pretrained_models', 'train_bst_model': '${paths.root_dir}/results/augumented-data-yolo12l/weights/final_best.pt', 'train_lst_model': '${paths.root_dir}/results/faugumented-data-yolo12l/weights/final_last.pt'}\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig\n",
    "\n",
    "with initialize(config_path=\"../configs\", job_name=\"EDA\", version_base=None):\n",
    "    cfg: DictConfig = compose(config_name=\"inference.yaml\")\n",
    "    # print(OmegaConf.to_yaml(cfg))\n",
    "    print(cfg.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d32eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b83c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7003581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path(cfg.paths.root_dir)\n",
    "ROOT_DATA_DIR = Path(cfg.paths.root_data_dir)\n",
    "DATA_DIR = ROOT_DATA_DIR / cfg.datasets.datasets_dir\n",
    "DATASET = cfg.datasets.roboflow\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_DIR = DATA_DIR / cfg.datasets.dataset_name\n",
    "CLASS_NAMES = cfg.datasets.names\n",
    "pred_args = hydra.utils.instantiate(cfg.args, _convert_=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c24d30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pred_args[\"imgsz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9dc210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/workspaces/football-players-tracking-yolo/results/augumented-data-yolo12l/weights/final_best.pt',\n",
       " '/workspaces/football-players-tracking-yolo/results/faugumented-data-yolo12l/weights/final_last.pt')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_BST_MODEL = cfg.paths.train_bst_model\n",
    "TRAIN_LST_MODEL = cfg.paths.train_lst_model\n",
    "TRAIN_BST_MODEL, TRAIN_LST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c297f",
   "metadata": {},
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O \"121364_0.mp4\" \"https://drive.google.com/uc?id=1vVwjW1dE1drIdd4ZSILfbCGPD4weoNiu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list(DATA_DIR.iterdir())) == 0:\n",
    "    from roboflow import Roboflow\n",
    "\n",
    "    rf = Roboflow()\n",
    "    project = rf.workspace(DATASET.workspace).project(DATASET.project)\n",
    "    version = project.version(DATASET.version)\n",
    "    dataset = version.download(model_format=DATASET.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955f810",
   "metadata": {},
   "source": [
    "## YOLO model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08140240",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BST_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14880cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(TRAIN_BST_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cf57c",
   "metadata": {},
   "source": [
    "## Inference on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_VIDEO = \"/workspaces/football-players-tracking-yolo/data/0bfacc_0.mp4\"\n",
    "TEST_VIDEO2 = \"/workspaces/football-players-tracking-yolo/data/121364_0.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5804980",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(TEST_VIDEO)\n",
    "frame = next(frame_generator)\n",
    "sv.plot_image(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "BALL_ID = 0\n",
    "colors_list = [sv.Color.RED, sv.Color.WHITE, sv.Color.GREEN, sv.Color.BLUE]\n",
    "colors = sv.ColorPalette(colors=colors_list)\n",
    "\n",
    "# ellip_annotator = sv.EllipseAnnotator(color=colors,thickness=2)\n",
    "ellip_annotator = sv.EllipseAnnotator(color=colors, thickness=2)\n",
    "traingle_annot = sv.TriangleAnnotator(color=colors_list[0], base=25, height=21, outline_thickness=1)\n",
    "label_annotator = sv.LabelAnnotator(color=colors, text_color=sv.Color.BLACK, text_position=sv.Position.TOP_CENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(frame, imgsz=pred_args[\"imgsz\"], conf=0.3)[0]\n",
    "detections = sv.Detections.from_ultralytics(result)\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_detections = detections[detections.class_id == BALL_ID]\n",
    "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "rest_detections = detections[detections.class_id != BALL_ID]\n",
    "rest_detections = rest_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "rest_detections.class_id -= 1\n",
    "\n",
    "labels = [f\"{class_name} {confidence:.2f}\" for class_name, confidence in zip(detections[\"class_name\"], detections.confidence, strict=False)]\n",
    "frame = frame.copy()\n",
    "annot_frame = ellip_annotator.annotate(scene=frame, detections=rest_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_frame = traingle_annot.annotate(scene=annot_frame, detections=ball_detections)\n",
    "annotated_frame = label_annotator.annotate(scene=annot_frame, detections=rest_detections, labels=labels)\n",
    "sv.plot_image(annot_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "    # https://hydra.cc/docs/advanced/instantiate_objects/overview/#parameter-conversion-strategies\n",
    "    result = model.predict(frame, imgsz=pred_args[\"imgsz\"], conf=0.3)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    ball_detections = detections[detections.class_id == BALL_ID]\n",
    "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "    rest_detections = detections[detections.class_id != BALL_ID]\n",
    "    rest_detections = rest_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "    rest_detections.class_id -= 1\n",
    "\n",
    "    labels = [f\"{class_name} {confidence:.2f}\" for class_name, confidence in zip(rest_detections[\"class_name\"], detections.confidence, strict=False)]\n",
    "    annot_frame = ellip_annotator.annotate(scene=frame.copy(), detections=rest_detections)\n",
    "    annot_frame = label_annotator.annotate(scene=annot_frame, detections=rest_detections, labels=labels)\n",
    "    annot_frame = traingle_annot.annotate(scene=annot_frame, detections=ball_detections)\n",
    "    return annot_frame\n",
    "\n",
    "\n",
    "# sv.process_video(\n",
    "#     source_path=TEST_VIDEO,\n",
    "#     target_path=\"/workspaces/football-players-tracking-yolo/results/result.mp4\",\n",
    "#     callback=callback\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ff16c",
   "metadata": {},
   "source": [
    "## Detecting Players in 2 teams and annoting them in 2 colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5578aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "PLAYER_ID = 2\n",
    "STRIDE = 20\n",
    "frame_generator = sv.get_video_frames_generator(TEST_VIDEO, stride=STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd393547",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_crop = []\n",
    "for frame in tqdm(frame_generator, desc=\"Collecting players crop\"):\n",
    "    result = model.predict(frame, imgsz=pred_args[\"imgsz\"], conf=0.3)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    # Non-maximum suppression (NMS) is a technique used in object detection to eliminate\n",
    "    # redundant bounding boxes for the same object, ensuring that each object is\n",
    "    # represented by a single, precise bounding box.\n",
    "    player_detections = detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "    player_detections = detections[detections.class_id == PLAYER_ID]\n",
    "    players_crop += [sv.crop_image(frame, xyxy) for xyxy in player_detections.xyxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.plot_images_grid(players_crop[:100], grid_size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a47edb",
   "metadata": {},
   "source": [
    "## SigLip to calculate embeddings for each crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73275df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, SiglipVisionModel\n",
    "\n",
    "# bug with siglib2 - https://github.com/huggingface/transformers/issues/36399\n",
    "SIGLIP_MODEL_PATH = \"google/siglip-base-patch16-224\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
    "embedding_processor = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)  # for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaca2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# AutoProcessor expects pillow images\n",
    "crops = [sv.cv2_to_pillow(crop) for crop in players_crop]\n",
    "batches = chunked(crops, BATCH_SIZE)\n",
    "data_list = []\n",
    "\n",
    "# reduce memory consumption and speed up inference\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(batches, desc=\"Extracting embeddings\"):\n",
    "        _batch = embedding_processor(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = embedding_model(**_batch)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        data_list.append(embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate(data_list, axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7dc8e0",
   "metadata": {},
   "source": [
    "Using UMAP, we project our embeddings from (N, 768) to (N, 3) and then perform a two-cluster division using KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "projections = reducer.fit_transform(data)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "team_labels = kmeans.fit_predict(projections)\n",
    "team_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ec19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with Plotly Express\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=projections[:, 0],\n",
    "    y=projections[:, 1],\n",
    "    z=projections[:, 2],\n",
    "    color=team_labels.astype(str),  # Color by cluster\n",
    "    title=\"Player Embedding Clusters\",\n",
    "    labels={\"color\": \"Team\"},\n",
    "    hover_name=[f\"Player {i}\" for i in range(len(team_labels))],  # Optional hover text\n",
    ")\n",
    "fig.update_traces(marker_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564d55a",
   "metadata": {},
   "source": [
    "## Add detect and track players separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976168d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.datasets.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BALL_ID = 0\n",
    "colors_list = [sv.Color.WHITE, sv.Color.GREEN, sv.Color.YELLOW]\n",
    "colors = sv.ColorPalette(colors=colors_list)\n",
    "\n",
    "ellip_annotator = sv.EllipseAnnotator(color=colors, thickness=2)\n",
    "traingle_annot = sv.TriangleAnnotator(color=sv.Color.RED, base=25, height=21, outline_thickness=1)\n",
    "label_annotator = sv.LabelAnnotator(color=colors, text_color=sv.Color.BLACK, text_position=sv.Position.TOP_CENTER)\n",
    "model = YOLO(TRAIN_BST_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e6f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(TEST_VIDEO)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = model(frame)[0]\n",
    "detections = sv.Detections.from_ultralytics(result)\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec567684",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_detections = detections[detections.class_id == BALL_ID]\n",
    "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "rest_detections = detections[detections.class_id != BALL_ID]\n",
    "rest_detections = rest_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "rest_detections.class_id -= 1\n",
    "\n",
    "rest_detections = tracker.update_with_detections(detections=rest_detections)\n",
    "\n",
    "labels = [f\"#{int(id)}\" for id in rest_detections.tracker_id]\n",
    "\n",
    "frame = frame.copy()\n",
    "annot_frame = ellip_annotator.annotate(scene=frame, detections=rest_detections)\n",
    "annot_frame = traingle_annot.annotate(scene=annot_frame, detections=ball_detections)\n",
    "annotated_frame = label_annotator.annotate(scene=annot_frame, detections=rest_detections, labels=labels)\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracker_callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "    # https://hydra.cc/docs/advanced/instantiate_objects/overview/#parameter-conversion-strategies\n",
    "    result = model.predict(frame, **pred_args)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    # --- ball detection ---\n",
    "    ball_detections = detections[detections.class_id == BALL_ID]\n",
    "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "    # --- Players detection ---\n",
    "    rest_detections = detections[detections.class_id != BALL_ID]\n",
    "    rest_detections = rest_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "    rest_detections.class_id -= 1\n",
    "\n",
    "    rest_detections = tracker.update_with_detections(detections=rest_detections)\n",
    "\n",
    "    labels = [f\"#{int(id)}\" for id in rest_detections.tracker_id]\n",
    "\n",
    "    frame = frame.copy()\n",
    "    annot_frame = ellip_annotator.annotate(scene=frame, detections=rest_detections)\n",
    "    annot_frame = traingle_annot.annotate(scene=annot_frame, detections=ball_detections)\n",
    "    annot_frame = label_annotator.annotate(scene=annot_frame, detections=rest_detections, labels=labels)\n",
    "    return annot_frame\n",
    "\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()\n",
    "\n",
    "# sv.process_video(\n",
    "#     source_path=TEST_VIDEO,\n",
    "#     target_path=\"/workspaces/football-players-tracking-yolo/results/result_tracker.mp4\",\n",
    "#     callback=tracker_callback,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f06b0",
   "metadata": {},
   "source": [
    "## Tracking players from both team separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['goalkeeper', 'team_player1','team_player2', 'referee']\n",
    "colors_list = [\n",
    "    sv.Color.YELLOW,  # Index 0 (Goalkeeper)\n",
    "    sv.Color.RED,  # Index 1 (Team 1)\n",
    "    sv.Color.GREEN,  # Index 2 (Team 2)\n",
    "    sv.Color.WHITE,  # Index 3 (Referee)\n",
    "]\n",
    "colors = sv.ColorPalette(colors=colors_list)\n",
    "\n",
    "ellip_annotator = sv.EllipseAnnotator(color=colors, thickness=2)\n",
    "traingle_annot = sv.TriangleAnnotator(color=sv.Color.RED, base=25, height=21, outline_thickness=1)\n",
    "label_annotator = sv.LabelAnnotator(color=colors, text_color=sv.Color.BLACK, text_position=sv.Position.TOP_CENTER)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Final Class IDs and corresponding labels/colors\n",
    "# We will map dynamically detected teams to IDs 2 and 3\n",
    "FINAL_CLASS_ID_GOALKEEPER = 0\n",
    "FINAL_CLASS_ID_TEAM_1 = 1\n",
    "FINAL_CLASS_ID_TEAM_2 = 2\n",
    "FINAL_CLASS_ID_REFEREE = 3\n",
    "\n",
    "# Original Model Output Class IDs\n",
    "BALL_ID = 0\n",
    "GOALKEEPER_ID = 1\n",
    "PLAYER_ID = 2\n",
    "REFEREE_ID = 3\n",
    "\n",
    "TEST_VIDEO_2 = \"/workspaces/football-players-tracking-yolo/data/121364_0.mp4\"\n",
    "\n",
    "SIGLIP_MODEL_PATH = \"google/siglip-base-patch16-224\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
    "embedding_processor = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)  # for feature extraction\n",
    "\n",
    "_umap = umap.UMAP(n_components=3, random_state=cfg.seed)\n",
    "_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_generator = sv.get_video_frames_generator(TEST_VIDEO_2, stride=30)\n",
    "\n",
    "crops = []\n",
    "for frame in tqdm(frame_generator, desc=\"collecting crops\"):\n",
    "    result = model.predict(frame, **pred_args)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    players_detections = detections[detections.class_id == PLAYER_ID]\n",
    "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
    "    crops += players_crops\n",
    "\n",
    "batches = chunked(crops, BATCH_SIZE)\n",
    "data_list = []\n",
    "\n",
    "# reduce memory consumption and speed up inference\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(batches, desc=\"Extracting embeddings\"):\n",
    "        _batch = embedding_processor(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = embedding_model(**_batch)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        data_list.append(embeddings.cpu().numpy())\n",
    "data = np.concatenate(data_list)\n",
    "_kmeans.fit(_umap.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Callback Function ---\n",
    "def separate_teams_callback(frame: np.ndarray, frame_index: int) -> np.ndarray:\n",
    "    _frame = frame.copy()\n",
    "\n",
    "    # --- Object Detection ---\n",
    "    result = model.predict(frame, **pred_args)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "    # --- Ball Detection ---\n",
    "    ball_detections = detections[detections.class_id == BALL_ID]\n",
    "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "    # --- Non-Ball Detections & Tracking ---\n",
    "    non_ball_detections = detections[detections.class_id != BALL_ID]\n",
    "    non_ball_detections = non_ball_detections.with_nms(\n",
    "        threshold=0.5,\n",
    "        class_agnostic=False,  # Usually better to do NMS per class\n",
    "    )\n",
    "    non_ball_detections = _tracker.update_with_detections(detections=non_ball_detections)\n",
    "\n",
    "    # --- Separate Detections by Original Class ---\n",
    "    goalkeepers_detections = non_ball_detections[non_ball_detections.class_id == GOALKEEPER_ID]\n",
    "    players_detections = non_ball_detections[non_ball_detections.class_id == PLAYER_ID]\n",
    "    referees_detections = non_ball_detections[non_ball_detections.class_id == REFEREE_ID]\n",
    "\n",
    "    # --- Team Separation for Players ---\n",
    "\n",
    "    players_crop = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
    "    crops = [sv.cv2_to_pillow(crop) for crop in players_crop]\n",
    "\n",
    "    batches = chunked(crops, BATCH_SIZE)\n",
    "    data_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in batches:\n",
    "            _batch = embedding_processor(images=batch, return_tensors=\"pt\").to(DEVICE)\n",
    "            outputs = embedding_model(**_batch)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "            data_list.append(embeddings.cpu().numpy())\n",
    "\n",
    "    if data_list:\n",
    "        projections = _umap.transform(np.concatenate(data_list))\n",
    "\n",
    "        # Cluster the projections\n",
    "        cluster_ids = _kmeans.predict(projections)  # IDs are 0 or 1\n",
    "\n",
    "        # Assign FINAL Class IDs (2 and 3) based on (potentially corrected) cluster IDs\n",
    "        players_detections.class_id = cluster_ids + FINAL_CLASS_ID_TEAM_1  # Map 0->1, 1->2\n",
    "\n",
    "    # --- Assign Final Class IDs ---\n",
    "    # Goalkeepers get final ID 1\n",
    "    goalkeepers_detections.class_id = np.full(len(goalkeepers_detections), FINAL_CLASS_ID_GOALKEEPER)\n",
    "    # Referees get final ID 4\n",
    "    referees_detections.class_id = np.full(len(referees_detections), FINAL_CLASS_ID_REFEREE)\n",
    "\n",
    "    # --- Merge all detections for annotation --\n",
    "    all_detections = sv.Detections.merge([goalkeepers_detections, players_detections, referees_detections])\n",
    "    # Ensure class_id is integer type for palette indexing\n",
    "    all_detections.class_id = all_detections.class_id.astype(int)\n",
    "\n",
    "    # --- Create Labels (Using Tracker ID) ---\n",
    "    labels = [f\"#{tid}\" for tid in all_detections.tracker_id]\n",
    "\n",
    "    # --- Annotation ---\n",
    "    # Annotate non-ball detections (Goalkeepers, Players, Referees) with ellipses\n",
    "    annot_frame = ellip_annotator.annotate(scene=_frame, detections=all_detections)\n",
    "\n",
    "    # Annotate the ball separately\n",
    "    annot_frame = traingle_annot.annotate(scene=annot_frame, detections=ball_detections)\n",
    "\n",
    "    # Add labels to non-ball detections\n",
    "    annot_frame = label_annotator.annotate(scene=annot_frame, detections=all_detections, labels=labels)\n",
    "    return annot_frame\n",
    "\n",
    "\n",
    "_tracker = sv.ByteTrack()\n",
    "_tracker.reset()\n",
    "\n",
    "output_file = Path(cfg.paths.results_dir) / \"result_seperate_teams.mp4\"\n",
    "output_file.unlink(missing_ok=True)  # Remove existing file if it exists\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=TEST_VIDEO_2,\n",
    "    target_path=str(output_file),\n",
    "    callback=separate_teams_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd940bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/roboflow/sports.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sports.common.team import TeamClassifier\n",
    "\n",
    "PLAYER_ID = 2\n",
    "STRIDE = 30\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(source_path=TEST_VIDEO_2, stride=STRIDE)\n",
    "\n",
    "crops = []\n",
    "for frame in tqdm(frame_generator, desc=\"collecting crops\"):\n",
    "    result = model.predict(frame, **pred_args)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "    players_detections = detections[detections.class_id == PLAYER_ID]\n",
    "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in detections.xyxy]\n",
    "    crops += players_crops\n",
    "\n",
    "team_classifier = TeamClassifier(device=\"cuda\")\n",
    "team_classifier.fit(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Callback Function ---\n",
    "def separate_teams_callback(frame: np.ndarray, frame_index: int) -> np.ndarray:\n",
    "    global previous_centroids  # Use global state for centroids across calls\n",
    "    _frame = frame.copy()\n",
    "\n",
    "    # --- Object Detection ---\n",
    "    result = model.predict(frame, **pred_args)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "    # --- Ball Detection ---\n",
    "    ball_detections = detections[detections.class_id == BALL_ID]\n",
    "    # Optional: Pad ball boxes if needed\n",
    "    # ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "    # --- Non-Ball Detections & Tracking ---\n",
    "    non_ball_detections = detections[detections.class_id != BALL_ID]\n",
    "    non_ball_detections = non_ball_detections.with_nms(\n",
    "        threshold=0.5,\n",
    "        class_agnostic=False,  # Usually better to do NMS per class\n",
    "    )\n",
    "    # Add tracker IDs\n",
    "    non_ball_detections = tracker.update_with_detections(detections=non_ball_detections)\n",
    "\n",
    "    # --- Separate Detections by Original Class ---\n",
    "    goalkeepers_detections = non_ball_detections[non_ball_detections.class_id == GOALKEEPER_ID]\n",
    "    players_detections = non_ball_detections[non_ball_detections.class_id == PLAYER_ID]\n",
    "    referees_detections = non_ball_detections[non_ball_detections.class_id == REFEREE_ID]\n",
    "\n",
    "    players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
    "    players_detections.class_id = team_classifier.predict(players_crops)\n",
    "\n",
    "    # --- Assign Final Class IDs ---\n",
    "    # Goalkeepers get final ID 1\n",
    "    goalkeepers_detections.class_id = np.full(len(goalkeepers_detections), FINAL_CLASS_ID_GOALKEEPER)\n",
    "    # Referees get final ID 4\n",
    "    referees_detections.class_id = np.full(len(referees_detections), FINAL_CLASS_ID_REFEREE)\n",
    "\n",
    "    # --- Merge all detections for annotation --\n",
    "    all_detections = sv.Detections.merge([goalkeepers_detections, players_detections, referees_detections])\n",
    "    # Ensure class_id is integer type for palette indexing\n",
    "    all_detections.class_id = all_detections.class_id.astype(int)\n",
    "\n",
    "    # --- Create Labels (Using Tracker ID) ---\n",
    "    labels = [f\"#{tid}\" for tid in all_detections.tracker_id]\n",
    "\n",
    "    # --- Annotation ---\n",
    "    annot_frame = _frame  # Start with a copy of the current frame\n",
    "\n",
    "    # Annotate non-ball detections (Goalkeepers, Players, Referees) with ellipses\n",
    "    annot_frame = ellip_annotator.annotate(scene=annot_frame, detections=all_detections)\n",
    "\n",
    "    # Annotate the ball separately\n",
    "    annot_frame = traingle_annot.annotate(scene=annot_frame, detections=ball_detections)\n",
    "\n",
    "    # Add labels to non-ball detections\n",
    "    annot_frame = label_annotator.annotate(scene=annot_frame, detections=all_detections, labels=labels)\n",
    "    return annot_frame\n",
    "\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()\n",
    "\n",
    "output_file = Path(cfg.paths.results_dir) / \"result_seperate_teams.mp4\"\n",
    "output_file.unlink(missing_ok=True)  # Remove existing file if it exists\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=TEST_VIDEO_2,\n",
    "    target_path=str(output_file),\n",
    "    callback=separate_teams_callback,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
